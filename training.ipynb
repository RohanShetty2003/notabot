{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNS4qAObwrvywpglZtcibMW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohanS2003/notabot/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwFULSk8lzim"
      },
      "outputs": [],
      "source": [
        "#Importing the necessary modules\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the VGG16\n",
        "model1 = VGG16()\n",
        "\n",
        "#Changing the model: Removing the predicted values from the existing VGG16 model\n",
        "model1 = Model(inputs=model1.inputs, outputs=model1.layers[-2].output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FJjSOmNoYVo",
        "outputId": "1b9b1090-48d5-4c86-93af-d4ba1f68eccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR=''\n",
        "directory = ''\n",
        "features = {}\n",
        "#directory = os.path.join(BASE_DIR, 'frames')\n",
        "\n",
        "for i in tqdm(os.listdir(directory)):\n",
        "    img_path = directory + '/' + i\n",
        "    image = load_img(img_path, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    image = preprocess_input(image)\n",
        "    feature = model1.predict(image, verbose=0)\n",
        "    image_id = i.split('.')[0]\n",
        "    features[image_id] = feature\n",
        "\n",
        "#pickle.dump(features, open(os.path.join(BASE_DIR, 'features.pkl'), 'wb'))"
      ],
      "metadata": {
        "id": "APTwZW8iodYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mapping the descriptions to the images\n",
        "mapping = {}\n",
        "for each_desc in tqdm(desc_doc.split('\\n')):\n",
        "    tokens = each_desc.split(',')\n",
        "    if len(each_desc) < 2:\n",
        "        continue\n",
        "    image_id, desc_of = tokens[0], tokens[1:]\n",
        "    image_id = image_id.split('.')[0]\n",
        "    desc_of = \" \".join(desc_of)\n",
        "    if image_id not in mapping:\n",
        "        mapping[image_id] = []\n",
        "    mapping[image_id].append(desc_of)"
      ],
      "metadata": {
        "id": "kB9skqzahSKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Editing the descriptions: Convert to lower case and add beginning and ending\n",
        "def edit_description(mapping):\n",
        "    for key, desc in mapping.items():\n",
        "        for i in range(len(desc)):\n",
        "            x = desc[i]\n",
        "            x = x.lower()\n",
        "            x = x.replace('[^A-Za-z]', '')\n",
        "            x = x.replace('\\s+', ' ')\n",
        "            x = 'beginning ' + \" \".join([word for word in x.split() if len(word)>1]) + ' ending'\n",
        "            desc[i] = x"
      ],
      "metadata": {
        "id": "7mmnP7SKhUZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the preprocessing text function\n",
        "edit_description(mapping)"
      ],
      "metadata": {
        "id": "RCS7nXLbhWPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Appending all descriptions into a list: Each image with 5 descriptions\n",
        "img_desc = []\n",
        "for key in mapping:\n",
        "    for caption in mapping[key]:\n",
        "        img_desc.append(caption)"
      ],
      "metadata": {
        "id": "hSQjeIv4hZtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the text: finding the unique words from all the captions\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(img_desc)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "0ih1Pfl7hbGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the maximum description length for the padding required\n",
        "max_length = max(len(text.split()) for text in img_desc)"
      ],
      "metadata": {
        "id": "x_oXUpTHhhAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into Training and Testing: 90% is given to training and remaining is for the test\n",
        "image_ids = list(mapping.keys())\n",
        "split = int(len(image_ids) * 0.90)\n",
        "train = image_ids[:split]\n",
        "test = image_ids[split:]"
      ],
      "metadata": {
        "id": "s6xhWCcdhkTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the data frm the inputs of images and descriptions and passing it for the model\n",
        "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n = 0\n",
        "    while 1:\n",
        "        for key in data_keys:\n",
        "            n += 1\n",
        "            text = mapping[key]\n",
        "            for t in text:\n",
        "                seq = tokenizer.texts_to_sequences([t])[0]\n",
        "                for i in range(1, len(seq)):\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    X1.append(features[key][0])\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            if n == batch_size:\n",
        "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
        "                yield [X1, X2], y\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n = 0"
      ],
      "metadata": {
        "id": "WWYQ8D7Th16A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Giving the inputs for the CNN\n",
        "\n",
        "inputs1 = Input(shape=(4096,))\n",
        "fe1 = Dropout(0.4)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.4)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "hTeP52t0h3gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model with 20 epochs\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "steps = len(train) // batch_size\n",
        "\n",
        "for i in range(epochs):\n",
        "    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "\n",
        "model.save('best_model.h5')"
      ],
      "metadata": {
        "id": "_iV9cSEoh901"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mapping_toword(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ],
      "metadata": {
        "id": "tFFu_MfVh-55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_description(model, image, tokenizer, max_length):\n",
        "    in_text = 'beginning'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], max_length)\n",
        "        desc_predict = model.predict([image, sequence], verbose=0)\n",
        "\n",
        "        desc_predict = np.argmax(desc_predict)\n",
        "        word = mapping_toword(desc_predict, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += \" \" + word\n",
        "        if word == 'ending':\n",
        "            break\n",
        "      \n",
        "    return in_text"
      ],
      "metadata": {
        "id": "W2Mt0qD2iD1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual, predicted = list(), list()\n",
        "\n",
        "for key in tqdm(test):\n",
        "    desc = mapping[key]\n",
        "    y_pred = predict_description(model, features[key], tokenizer, max_length) \n",
        "    actual_desc = [caption.split() for text in desc]\n",
        "    y_pred = y_pred.split()\n",
        "    actual.append(actual_desc)\n",
        "    predicted.append(y_pred)"
      ],
      "metadata": {
        "id": "Cs0bXWaUiFhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyttsx3"
      ],
      "metadata": {
        "id": "PwKcFzMFiMBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "def generate_text(image_name):\n",
        "    image_id = image_name.split('.')[0]\n",
        "    img_path = os.path.join(BASE_DIR, \"Images\", image_name)\n",
        "    image = Image.open(img_path)\n",
        "    desc = mapping[image_id]\n",
        "    y_pred = predict_description(model, features[image_id], tokenizer, max_length)\n",
        "    plt.imshow(image)\n",
        "\n",
        "    return str(y_pred)"
      ],
      "metadata": {
        "id": "Yho3nIQ6iNGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the required modules\n",
        "\n",
        "!pip3 install gTTS pyttsx3 playsound pygobject"
      ],
      "metadata": {
        "id": "6fzWzfnfiORM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install SpeechRecognition"
      ],
      "metadata": {
        "id": "FfZzRxPRiRyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the engine for voice to text for input commands\n",
        "import os\n",
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "\n",
        "def voice_output(command):\n",
        "    engine = pyttsx3.init()\n",
        "    engine.say(command)\n",
        "    engine.runAndWait()\n",
        "r = sr.Recognizer()\n",
        "x = 0\n"
      ],
      "metadata": {
        "id": "xI4uUdAAiYHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTs"
      ],
      "metadata": {
        "id": "d6srhHnZiayI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the trained model\n",
        "from tensorflow import keras\n",
        "#model = keras.models.load_model('/path/to/trained/model.h5')\n",
        "\n",
        "# load the image\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "image_path = '/kaggle/input/flickr8k/Images/1032460886_4a598ed535.jpg'\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "image = img_to_array(image)\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# extract features using VGG16 model\n",
        "feature_vector = model1.predict(image, verbose=0)\n",
        "\n",
        "# generate the caption\n",
        "'''def predict_caption(model, tokenizer, feature_vector, max_length):\n",
        "    in_text = 'beginning'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([feature_vector,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = mapping_toword(yhat, tokenizer)\n",
        "        in_text += ' ' + word\n",
        "        if word == 'ending':\n",
        "            break\n",
        "    return in_text'''\n",
        "\n",
        "def predict_caption(model, tokenizer, feature_vector, max_length):\n",
        "    in_text = 'beginning'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.encode(in_text)\n",
        "        sequence = np.array(sequence).reshape(1,-1)\n",
        "        yhat = model.predict([feature_vector,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = tokenizer.decode([yhat])\n",
        "        in_text += ' ' + word\n",
        "        if word == 'ending':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "\n",
        "caption = predict_caption(model, tokenizer, feature_vector, max_length)\n"
      ],
      "metadata": {
        "id": "lSKl9gGfiggr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(caption)\n",
        "\n",
        "image = Image.open(image_path)\n",
        "plt.imshow(image)\n",
        "\n",
        "res = caption.split(' ', 1)[1]\n",
        "text = res.rsplit(' ', 1)[0]\n",
        "\n",
        "tts = gTTS(text) \n",
        "\n",
        "tts.save('info.wav')\n",
        "sound_file = 'info.wav'\n",
        "Audio(sound_file, autoplay=True)"
      ],
      "metadata": {
        "id": "QhwUTaFQih1Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}