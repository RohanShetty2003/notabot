{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# # Smart Voice Assistant For The Blind","metadata":{}},{"cell_type":"code","source":"#Importing the necessary modules\nimport os\nimport pickle\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical, plot_model\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-15T23:01:58.851789Z","iopub.execute_input":"2023-04-15T23:01:58.852195Z","iopub.status.idle":"2023-04-15T23:02:04.504723Z","shell.execute_reply.started":"2023-04-15T23:01:58.852112Z","shell.execute_reply":"2023-04-15T23:02:04.503723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the VGG16\nmodel1 = VGG16()\n\n#Changing the model: Removing the predicted values from the existing VGG16 model\nmodel1 = Model(inputs=model1.inputs, outputs=model1.layers[-2].output)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:02:09.478768Z","iopub.execute_input":"2023-04-15T23:02:09.480133Z","iopub.status.idle":"2023-04-15T23:02:16.231508Z","shell.execute_reply.started":"2023-04-15T23:02:09.480091Z","shell.execute_reply":"2023-04-15T23:02:16.230547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport os\n\ndef extract_frames(video_path):\n    # Open the video file\n    cap = cv2.VideoCapture(video_path)\n\n    # Get the video frame rate and total number of frames\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Calculate the time interval between frames\n    interval = int(fps * 10)\n\n    # Create a directory to save the frames\n    output_dir = '/kaggle/working/' + \"frames\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Loop through the frames and extract every 10th frame\n    frame_num = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if ret:\n            if frame_num % interval == 0:\n                output_path = os.path.join(output_dir, f\"frame_{frame_num}.jpg\")\n                cv2.imwrite(output_path, frame)\n            frame_num += 1\n        else:\n            break\n\n    # Release the video capture and destroy any remaining windows\n    cap.release()\n    #cv2.destroyAllWindows()\n\n    print(f\"Successfully extracted {frame_num//interval} frames from {video_path} into {output_dir}\")\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T19:55:04.003105Z","iopub.execute_input":"2023-04-14T19:55:04.003711Z","iopub.status.idle":"2023-04-14T19:55:04.206483Z","shell.execute_reply.started":"2023-04-14T19:55:04.003666Z","shell.execute_reply":"2023-04-14T19:55:04.205319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install opencv-python-headless==4.5.4.58 opencv-contrib-python-headless==4.5.4.58\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extract_frames(\"/kaggle/input/sample-video/SampleVideo_1280x720_30mb.mp4\")","metadata":{"execution":{"iopub.status.busy":"2023-04-14T19:55:13.73658Z","iopub.execute_input":"2023-04-14T19:55:13.73696Z","iopub.status.idle":"2023-04-14T19:55:24.275361Z","shell.execute_reply.started":"2023-04-14T19:55:13.736925Z","shell.execute_reply":"2023-04-14T19:55:24.274183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_DIR='/kaggle/input/flickr8k'\ndirectory = '/kaggle/input/tcf-images'\nfeatures = {}\n#directory = os.path.join(BASE_DIR, 'frames')\n\nfor i in tqdm(os.listdir(directory)):\n    img_path = directory + '/' + i\n    image = load_img(img_path, target_size=(224, 224))\n    image = img_to_array(image)\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    image = preprocess_input(image)\n    feature = model1.predict(image, verbose=0)\n    image_id = i.split('.')[0]\n    features[image_id] = feature\n\n#pickle.dump(features, open(os.path.join(BASE_DIR, 'features.pkl'), 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-04-15T17:50:32.994586Z","iopub.execute_input":"2023-04-15T17:50:32.994964Z","iopub.status.idle":"2023-04-15T17:50:42.215993Z","shell.execute_reply.started":"2023-04-15T17:50:32.994933Z","shell.execute_reply":"2023-04-15T17:50:42.215105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load features from the saved pickle file\nwith open(os.path.join('', 'features.pkl'), 'rb') as f:\n    features = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T19:51:07.70631Z","iopub.execute_input":"2023-04-14T19:51:07.706683Z","iopub.status.idle":"2023-04-14T19:51:07.730656Z","shell.execute_reply.started":"2023-04-14T19:51:07.706651Z","shell.execute_reply":"2023-04-14T19:51:07.729543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the descriptions.txt file\nwith open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n    next(f)\n    desc_doc = f.read()","metadata":{"execution":{"iopub.status.busy":"2023-04-15T18:09:06.719685Z","iopub.execute_input":"2023-04-15T18:09:06.720395Z","iopub.status.idle":"2023-04-15T18:09:06.797662Z","shell.execute_reply.started":"2023-04-15T18:09:06.720356Z","shell.execute_reply":"2023-04-15T18:09:06.796618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Mapping the descriptions to the images\nmapping = {}\nfor each_desc in tqdm(desc_doc.split('\\n')):\n    tokens = each_desc.split(',')\n    if len(each_desc) < 2:\n        continue\n    image_id, desc_of = tokens[0], tokens[1:]\n    image_id = image_id.split('.')[0]\n    desc_of = \" \".join(desc_of)\n    if image_id not in mapping:\n        mapping[image_id] = []\n    mapping[image_id].append(desc_of)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T18:09:10.53244Z","iopub.execute_input":"2023-04-15T18:09:10.532823Z","iopub.status.idle":"2023-04-15T18:09:10.659014Z","shell.execute_reply.started":"2023-04-15T18:09:10.532789Z","shell.execute_reply":"2023-04-15T18:09:10.657802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Editing the descriptions: Convert to lower case and add beginning and ending\ndef edit_description(mapping):\n    for key, desc in mapping.items():\n        for i in range(len(desc)):\n            x = desc[i]\n            x = x.lower()\n            x = x.replace('[^A-Za-z]', '')\n            x = x.replace('\\s+', ' ')\n            x = 'beginning ' + \" \".join([word for word in x.split() if len(word)>1]) + ' ending'\n            desc[i] = x","metadata":{"execution":{"iopub.status.busy":"2023-04-15T18:09:17.379679Z","iopub.execute_input":"2023-04-15T18:09:17.380371Z","iopub.status.idle":"2023-04-15T18:09:17.387082Z","shell.execute_reply.started":"2023-04-15T18:09:17.380333Z","shell.execute_reply":"2023-04-15T18:09:17.386034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calling the preprocessing text function\nedit_description(mapping)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T18:09:20.465067Z","iopub.execute_input":"2023-04-15T18:09:20.465423Z","iopub.status.idle":"2023-04-15T18:09:20.593639Z","shell.execute_reply.started":"2023-04-15T18:09:20.465393Z","shell.execute_reply":"2023-04-15T18:09:20.592784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Appending all descriptions into a list: Each image with 5 descriptions\nimg_desc = []\nfor key in mapping:\n    for caption in mapping[key]:\n        img_desc.append(caption)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T18:09:21.609814Z","iopub.execute_input":"2023-04-15T18:09:21.610181Z","iopub.status.idle":"2023-04-15T18:09:21.62509Z","shell.execute_reply.started":"2023-04-15T18:09:21.610149Z","shell.execute_reply":"2023-04-15T18:09:21.624044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:06:50.307428Z","iopub.execute_input":"2023-04-15T23:06:50.307794Z","iopub.status.idle":"2023-04-15T23:06:50.928418Z","shell.execute_reply.started":"2023-04-15T23:06:50.307765Z","shell.execute_reply":"2023-04-15T23:06:50.927473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenizing the text: finding the unique words from all the captions\ntokenizer=GPT2Tokenizer.from_pretrained('gpt2-medium')\nvocab_size = len(tokenizer) + 1","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:19:13.607454Z","iopub.execute_input":"2023-04-15T23:19:13.607823Z","iopub.status.idle":"2023-04-15T23:19:15.245799Z","shell.execute_reply.started":"2023-04-15T23:19:13.607794Z","shell.execute_reply":"2023-04-15T23:19:15.244754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenizing the text: finding the unique words from all the captions\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(img_desc)\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:02:57.474944Z","iopub.execute_input":"2023-04-15T23:02:57.475516Z","iopub.status.idle":"2023-04-15T23:02:57.485861Z","shell.execute_reply.started":"2023-04-15T23:02:57.475473Z","shell.execute_reply":"2023-04-15T23:02:57.484501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tokenizer()","metadata":{"execution":{"iopub.status.busy":"2023-04-15T18:10:08.130439Z","iopub.execute_input":"2023-04-15T18:10:08.13082Z","iopub.status.idle":"2023-04-15T18:10:08.138007Z","shell.execute_reply.started":"2023-04-15T18:10:08.130787Z","shell.execute_reply":"2023-04-15T18:10:08.136951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Unique words in the captions are: \" + str(vocab_size))","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:19:18.678149Z","iopub.execute_input":"2023-04-15T23:19:18.678543Z","iopub.status.idle":"2023-04-15T23:19:18.684588Z","shell.execute_reply.started":"2023-04-15T23:19:18.67851Z","shell.execute_reply":"2023-04-15T23:19:18.683355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the maximum description length for the padding required\nmax_length = max(len(text.split()) for text in img_desc)\nmax_length","metadata":{"execution":{"iopub.status.busy":"2023-04-15T18:10:34.81277Z","iopub.execute_input":"2023-04-15T18:10:34.813453Z","iopub.status.idle":"2023-04-15T18:10:34.848675Z","shell.execute_reply.started":"2023-04-15T18:10:34.813415Z","shell.execute_reply":"2023-04-15T18:10:34.847609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:19:43.667031Z","iopub.execute_input":"2023-04-15T23:19:43.667427Z","iopub.status.idle":"2023-04-15T23:19:43.690089Z","shell.execute_reply.started":"2023-04-15T23:19:43.667393Z","shell.execute_reply":"2023-04-15T23:19:43.688647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the dataset into Training and Testing: 90% is given to training and remaining is for the test\nimage_ids = list(mapping.keys())\nsplit = int(len(image_ids) * 0.90)\ntrain = image_ids[:split]\ntest = image_ids[split:]","metadata":{"execution":{"iopub.status.busy":"2023-04-15T18:10:39.332374Z","iopub.execute_input":"2023-04-15T18:10:39.332765Z","iopub.status.idle":"2023-04-15T18:10:39.338926Z","shell.execute_reply.started":"2023-04-15T18:10:39.332713Z","shell.execute_reply":"2023-04-15T18:10:39.337717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generating the data frm the inputs of images and descriptions and passing it for the model\ndef data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n    X1, X2, y = list(), list(), list()\n    n = 0\n    while 1:\n        for key in data_keys:\n            n += 1\n            text = mapping[key]\n            for t in text:\n                seq = tokenizer.texts_to_sequences([t])[0]\n                for i in range(1, len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                    X1.append(features[key][0])\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            if n == batch_size:\n                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                yield [X1, X2], y\n                X1, X2, y = list(), list(), list()\n                n = 0","metadata":{"execution":{"iopub.status.busy":"2023-04-15T18:10:41.476641Z","iopub.execute_input":"2023-04-15T18:10:41.477331Z","iopub.status.idle":"2023-04-15T18:10:41.486311Z","shell.execute_reply.started":"2023-04-15T18:10:41.477294Z","shell.execute_reply":"2023-04-15T18:10:41.485038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Giving the inputs for the CNN\n\ninputs1 = Input(shape=(4096,))\nfe1 = Dropout(0.4)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\n\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\nse2 = Dropout(0.4)(se1)\nse3 = LSTM(256)(se2)\n\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2023-04-14T19:52:26.928265Z","iopub.execute_input":"2023-04-14T19:52:26.928671Z","iopub.status.idle":"2023-04-14T19:52:27.791603Z","shell.execute_reply.started":"2023-04-14T19:52:26.928637Z","shell.execute_reply":"2023-04-14T19:52:27.790542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training the model with 20 epochs\nepochs = 20\nbatch_size = 32\nsteps = len(train) // batch_size\n\nfor i in range(epochs):\n    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n\nmodel.save('best_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nmodel = keras.models.load_model('/kaggle/input/modelv1/best_model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:11:34.629034Z","iopub.execute_input":"2023-04-15T23:11:34.629441Z","iopub.status.idle":"2023-04-15T23:11:36.342178Z","shell.execute_reply.started":"2023-04-15T23:11:34.629405Z","shell.execute_reply":"2023-04-15T23:11:36.341165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mapping_toword(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:11:41.375699Z","iopub.execute_input":"2023-04-15T23:11:41.376834Z","iopub.status.idle":"2023-04-15T23:11:41.382918Z","shell.execute_reply.started":"2023-04-15T23:11:41.376788Z","shell.execute_reply":"2023-04-15T23:11:41.3817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_description(model, image, tokenizer, max_length):\n    in_text = 'beginning'\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], max_length)\n        desc_predict = model.predict([image, sequence], verbose=0)\n\n        desc_predict = np.argmax(desc_predict)\n        word = mapping_toword(desc_predict, tokenizer)\n        if word is None:\n            break\n        in_text += \" \" + word\n        if word == 'ending':\n            break\n      \n    return in_text","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:11:46.029365Z","iopub.execute_input":"2023-04-15T23:11:46.029813Z","iopub.status.idle":"2023-04-15T23:11:46.037533Z","shell.execute_reply.started":"2023-04-15T23:11:46.029774Z","shell.execute_reply":"2023-04-15T23:11:46.036573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual, predicted = list(), list()\n\nfor key in tqdm(test):\n    desc = mapping[key]\n    y_pred = predict_description(model, features[key], tokenizer, max_length) \n    actual_desc = [caption.split() for text in desc]\n    y_pred = y_pred.split()\n    actual.append(actual_desc)\n    predicted.append(y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T19:52:42.393444Z","iopub.execute_input":"2023-04-14T19:52:42.393879Z","iopub.status.idle":"2023-04-14T19:52:42.46153Z","shell.execute_reply.started":"2023-04-14T19:52:42.393845Z","shell.execute_reply":"2023-04-14T19:52:42.459641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer=Tokenizer()","metadata":{"execution":{"iopub.status.busy":"2023-04-15T17:56:45.62693Z","iopub.execute_input":"2023-04-15T17:56:45.627848Z","iopub.status.idle":"2023-04-15T17:56:45.633933Z","shell.execute_reply.started":"2023-04-15T17:56:45.627769Z","shell.execute_reply":"2023-04-15T17:56:45.632862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyttsx3","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:11:58.034262Z","iopub.execute_input":"2023-04-15T23:11:58.034634Z","iopub.status.idle":"2023-04-15T23:12:08.507953Z","shell.execute_reply.started":"2023-04-15T23:11:58.034603Z","shell.execute_reply":"2023-04-15T23:12:08.506737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\ndef generate_text(image_name):\n    image_id = image_name.split('.')[0]\n    img_path = os.path.join(BASE_DIR, \"Images\", image_name)\n    image = Image.open(img_path)\n    desc = mapping[image_id]\n    y_pred = predict_description(model, features[image_id], tokenizer, max_length)\n    plt.imshow(image)\n\n    return str(y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T19:52:50.097368Z","iopub.execute_input":"2023-04-14T19:52:50.098018Z","iopub.status.idle":"2023-04-14T19:52:50.105425Z","shell.execute_reply.started":"2023-04-14T19:52:50.097981Z","shell.execute_reply":"2023-04-14T19:52:50.104227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Installing the required modules\n\n!pip3 install gTTS pyttsx3 playsound pygobject","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:12:44.816705Z","iopub.execute_input":"2023-04-15T23:12:44.817097Z","iopub.status.idle":"2023-04-15T23:13:25.891456Z","shell.execute_reply.started":"2023-04-15T23:12:44.817064Z","shell.execute_reply":"2023-04-15T23:13:25.890263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install SpeechRecognition","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:13:31.592345Z","iopub.execute_input":"2023-04-15T23:13:31.592751Z","iopub.status.idle":"2023-04-15T23:13:41.936455Z","shell.execute_reply.started":"2023-04-15T23:13:31.592719Z","shell.execute_reply":"2023-04-15T23:13:41.935383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up the engine for voice to text for input commands\nimport os\nimport speech_recognition as sr\nimport pyttsx3\n\ndef voice_output(command):\n    engine = pyttsx3.init()\n    engine.say(command)\n    engine.runAndWait()\nr = sr.Recognizer()\nx = 0\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:13:44.920142Z","iopub.execute_input":"2023-04-15T23:13:44.920543Z","iopub.status.idle":"2023-04-15T23:13:44.941189Z","shell.execute_reply.started":"2023-04-15T23:13:44.920507Z","shell.execute_reply":"2023-04-15T23:13:44.940309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gTTs","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:13:46.836527Z","iopub.execute_input":"2023-04-15T23:13:46.836876Z","iopub.status.idle":"2023-04-15T23:13:56.461585Z","shell.execute_reply.started":"2023-04-15T23:13:46.836848Z","shell.execute_reply":"2023-04-15T23:13:56.460256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gtts import gTTS\nfrom IPython.display import Audio\n\ntext = str(generate_text(\"23445819_3a458716c1.jpg\"))\nprint(text)\n\nres = text.split(' ', 1)[1]\ntext = res.rsplit(' ', 1)[0]\n\ntts = gTTS(text) \n\ntts.save('info.wav')\nsound_file = 'info.wav'\nAudio(sound_file, autoplay=True) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = str(generate_text(\"1001773457_577c3a7d70.jpg\"))\nprint(text)\n\nres = text.split(' ', 1)[1]\ntext = res.rsplit(' ', 1)[0]\n\ntts = gTTS(text) \n\ntts.save('info.wav')\nsound_file = 'info.wav'\nAudio(sound_file, autoplay=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = str(generate_text(\"1002674143_1b742ab4b8.jpg\"))\nprint(text)\n\nres = text.split(' ', 1)[1]\ntext = res.rsplit(' ', 1)[0]\n\ntts = gTTS(text) \n\ntts.save('info.wav')\nsound_file = 'info.wav'\nAudio(sound_file, autoplay=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length=35","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:21:03.826455Z","iopub.execute_input":"2023-04-15T23:21:03.826812Z","iopub.status.idle":"2023-04-15T23:21:03.831873Z","shell.execute_reply.started":"2023-04-15T23:21:03.826782Z","shell.execute_reply":"2023-04-15T23:21:03.83061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the trained model\nfrom tensorflow import keras\n#model = keras.models.load_model('/path/to/trained/model.h5')\n\n# load the image\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimage_path = '/kaggle/input/flickr8k/Images/1032460886_4a598ed535.jpg'\nimage = load_img(image_path, target_size=(224, 224))\nimage = img_to_array(image)\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\nimage = preprocess_input(image)\n\n# extract features using VGG16 model\nfeature_vector = model1.predict(image, verbose=0)\n\n# generate the caption\n'''def predict_caption(model, tokenizer, feature_vector, max_length):\n    in_text = 'beginning'\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = model.predict([feature_vector,sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = mapping_toword(yhat, tokenizer)\n        in_text += ' ' + word\n        if word == 'ending':\n            break\n    return in_text'''\n\ndef predict_caption(model, tokenizer, feature_vector, max_length):\n    in_text = 'beginning'\n    for i in range(max_length):\n        sequence = tokenizer.encode(in_text)\n        sequence = np.array(sequence).reshape(1,-1)\n        yhat = model.predict([feature_vector,sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = tokenizer.decode([yhat])\n        in_text += ' ' + word\n        if word == 'ending':\n            break\n    return in_text\n\n\ncaption = predict_caption(model, tokenizer, feature_vector, max_length)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-15T23:21:06.270375Z","iopub.execute_input":"2023-04-15T23:21:06.270982Z","iopub.status.idle":"2023-04-15T23:21:06.4257Z","shell.execute_reply.started":"2023-04-15T23:21:06.270945Z","shell.execute_reply":"2023-04-15T23:21:06.424187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(caption)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T18:11:57.630357Z","iopub.execute_input":"2023-04-15T18:11:57.630791Z","iopub.status.idle":"2023-04-15T18:11:57.637008Z","shell.execute_reply.started":"2023-04-15T18:11:57.630727Z","shell.execute_reply":"2023-04-15T18:11:57.635729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gtts import gTTS\nfrom IPython.display import Audio\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nprint(caption)\n\nimage = Image.open(image_path)\nplt.imshow(image)\n\nres = caption.split(' ', 1)[1]\ntext = res.rsplit(' ', 1)[0]\n\ntts = gTTS(text) \n\ntts.save('info.wav')\nsound_file = 'info.wav'\nAudio(sound_file, autoplay=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-15T18:22:15.535587Z","iopub.execute_input":"2023-04-15T18:22:15.536614Z","iopub.status.idle":"2023-04-15T18:22:16.019476Z","shell.execute_reply.started":"2023-04-15T18:22:15.536577Z","shell.execute_reply":"2023-04-15T18:22:16.018592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}